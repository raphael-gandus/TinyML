# Projet 1 | Raphaël GANDUS

J'ai suivi les mêmes étapes qu'en cours avec les exercices du git suivant : https://github.com/walid213/tinyml-workshop

Sauf qu'au lieu d'enregistrer des mouvements de coups de poings (punch et flex) j'ai effectué des mouvements correspondants aux emojis yes et no.

Cependant pour l'emoji no je n'ai pas fait une croix car cela ressemblait trop au yes donc j'ai fait deux lignes horizontales de gauche à droite puis de droite à gauche.

Vous trouverez dans Projet 1 le fichier "model.h" d'entrainement avec les fichiers "yes.csv" et "no.csv", le code source IMU_Classifier.ino utilisé pour le test et les questions, et le fichier colab utilisé pour créer "model.h".

Résultat obtenu : 
![image](https://user-images.githubusercontent.com/95090973/211158129-adf6e45c-cf85-405b-a54f-a15fd0678732.png)



# Projet 2 | Raphaël GANDUS et Hicham GHANEM

Avec mon camarade Hicham nous avons réussi à entrainer un modèle grâce à la plateforme Edge Impulse qui fonctionne à peu près correctement mais il manque sûrement plusieurs dizaines de minutes d'audio pour l'entrainement que nous n'avons pas eu le temps d'enregistrer. Vous trouverez dans Projet 2 le fichier flash pour l'Arduino produit par Edge Impulse.

Résultat : 
![image](https://user-images.githubusercontent.com/95090973/211208929-a826238c-f601-4252-9b21-e2ba04412582.png)


